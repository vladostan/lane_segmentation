Date and time: 2019-01-11 15-15-40

Datasets used: ['/home/v.ostankovich/lanes-segmentation/data/images/innopolis']

633
633
Image dtype:uint8
Label dtype:uint8

506
506
127
127
Class weights: [0.16615753 1.         1.43011478]

Model summary:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 256, 640, 3)  0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 256, 640, 64) 1792        input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 256, 640, 64) 36928       conv2d_1[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 128, 320, 64) 0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 128, 320, 128 73856       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 128, 320, 128 147584      conv2d_3[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 64, 160, 128) 0           conv2d_4[0][0]                   
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 64, 160, 256) 295168      max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 64, 160, 256) 590080      conv2d_5[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 32, 80, 256)  0           conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 80, 512)  1180160     max_pooling2d_3[0][0]            
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 80, 512)  2359808     conv2d_7[0][0]                   
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 32, 80, 512)  0           conv2d_8[0][0]                   
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 16, 40, 512)  0           dropout_1[0][0]                  
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 16, 40, 1024) 4719616     max_pooling2d_4[0][0]            
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 16, 40, 1024) 9438208     conv2d_9[0][0]                   
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 16, 40, 1024) 0           conv2d_10[0][0]                  
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 32, 80, 1024) 0           dropout_2[0][0]                  
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 80, 512)  2097664     up_sampling2d_1[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 32, 80, 1024) 0           dropout_1[0][0]                  
                                                                 conv2d_11[0][0]                  
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 32, 80, 512)  4719104     concatenate_1[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 32, 80, 512)  2359808     conv2d_12[0][0]                  
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 64, 160, 512) 0           conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 64, 160, 256) 524544      up_sampling2d_2[0][0]            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 64, 160, 512) 0           conv2d_6[0][0]                   
                                                                 conv2d_14[0][0]                  
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 64, 160, 256) 1179904     concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 64, 160, 256) 590080      conv2d_15[0][0]                  
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, 128, 320, 256 0           conv2d_16[0][0]                  
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 128, 320, 128 131200      up_sampling2d_3[0][0]            
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 128, 320, 256 0           conv2d_4[0][0]                   
                                                                 conv2d_17[0][0]                  
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 128, 320, 128 295040      concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 128, 320, 128 147584      conv2d_18[0][0]                  
__________________________________________________________________________________________________
up_sampling2d_4 (UpSampling2D)  (None, 256, 640, 128 0           conv2d_19[0][0]                  
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 256, 640, 64) 32832       up_sampling2d_4[0][0]            
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 256, 640, 128 0           conv2d_2[0][0]                   
                                                                 conv2d_20[0][0]                  
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 256, 640, 64) 73792       concatenate_4[0][0]              
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 256, 640, 64) 36928       conv2d_21[0][0]                  
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 256, 640, 3)  195         conv2d_22[0][0]                  
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 256, 640, 3)  0           conv2d_23[0][0]                  
==================================================================================================
Total params: 31,031,875
Trainable params: 31,031,875
Non-trainable params: 0
__________________________________________________________________________________________________
Optimizer: <keras.optimizers.Adam object at 0x7fef98f8bad0>, learning rate: 0.0001, loss: categorical_crossentropy, metrics: ['accuracy']

Callbacks: [<keras.callbacks.ModelCheckpoint object at 0x7fef98f8b890>, <keras.callbacks.ReduceLROnPlateau object at 0x7fef98f8b250>, <keras.callbacks.EarlyStopping object at 0x7fef98f8ba10>, <keras.callbacks.CSVLogger object at 0x7fef98faa6d0>]

Steps per epoch: 506
Validation steps: 127

Starting training...

Epoch 1/1000
 - 94s - loss: 0.4816 - acc: 0.8141 - val_loss: 0.3762 - val_acc: 0.8274

Epoch 00001: val_loss improved from inf to 0.37622, saving model to weights/innopolis/2019-01-11 15-15-40.hdf5
Epoch 2/1000
 - 87s - loss: 0.3550 - acc: 0.8448 - val_loss: 0.3364 - val_acc: 0.8502

Epoch 00002: val_loss improved from 0.37622 to 0.33645, saving model to weights/innopolis/2019-01-11 15-15-40.hdf5
Epoch 3/1000
 - 87s - loss: 0.3371 - acc: 0.8489 - val_loss: 0.3373 - val_acc: 0.8483

Epoch 00003: val_loss did not improve from 0.33645
Epoch 4/1000
 - 87s - loss: 0.3208 - acc: 0.8515 - val_loss: 0.3921 - val_acc: 0.8146

Epoch 00004: val_loss did not improve from 0.33645
Epoch 5/1000
 - 88s - loss: 0.3133 - acc: 0.8517 - val_loss: 0.3117 - val_acc: 0.8518

Epoch 00005: val_loss improved from 0.33645 to 0.31167, saving model to weights/innopolis/2019-01-11 15-15-40.hdf5
Epoch 6/1000
 - 89s - loss: 0.2969 - acc: 0.8560 - val_loss: 0.2837 - val_acc: 0.8607

Epoch 00006: val_loss improved from 0.31167 to 0.28370, saving model to weights/innopolis/2019-01-11 15-15-40.hdf5
Epoch 7/1000
 - 87s - loss: 0.2845 - acc: 0.8591 - val_loss: 0.2982 - val_acc: 0.8524

Epoch 00007: val_loss did not improve from 0.28370
Epoch 8/1000
 - 88s - loss: 0.2761 - acc: 0.8752 - val_loss: 0.2618 - val_acc: 0.8932

Epoch 00008: val_loss improved from 0.28370 to 0.26179, saving model to weights/innopolis/2019-01-11 15-15-40.hdf5
Epoch 9/1000
 - 88s - loss: 0.2452 - acc: 0.9041 - val_loss: 0.2666 - val_acc: 0.8850

Epoch 00009: val_loss did not improve from 0.26179
Epoch 10/1000
 - 88s - loss: 0.2230 - acc: 0.9120 - val_loss: 0.2398 - val_acc: 0.9006

Epoch 00010: val_loss improved from 0.26179 to 0.23981, saving model to weights/innopolis/2019-01-11 15-15-40.hdf5
Epoch 11/1000
 - 89s - loss: 0.2046 - acc: 0.9191 - val_loss: 0.2480 - val_acc: 0.9000

Epoch 00011: val_loss did not improve from 0.23981
Epoch 12/1000
 - 87s - loss: 0.1974 - acc: 0.9230 - val_loss: 0.2321 - val_acc: 0.9117

Epoch 00012: val_loss improved from 0.23981 to 0.23207, saving model to weights/innopolis/2019-01-11 15-15-40.hdf5
Epoch 13/1000
 - 87s - loss: 0.1842 - acc: 0.9280 - val_loss: 0.2254 - val_acc: 0.9133

Epoch 00013: val_loss improved from 0.23207 to 0.22544, saving model to weights/innopolis/2019-01-11 15-15-40.hdf5
Epoch 14/1000
 - 86s - loss: 0.1692 - acc: 0.9337 - val_loss: 0.2374 - val_acc: 0.9159

Epoch 00014: val_loss did not improve from 0.22544
Epoch 15/1000
 - 87s - loss: 0.1596 - acc: 0.9374 - val_loss: 0.2102 - val_acc: 0.9202

Epoch 00015: val_loss improved from 0.22544 to 0.21018, saving model to weights/innopolis/2019-01-11 15-15-40.hdf5
Epoch 16/1000
 - 87s - loss: 0.1488 - acc: 0.9422 - val_loss: 0.2398 - val_acc: 0.9139

Epoch 00016: val_loss did not improve from 0.21018
Epoch 17/1000
 - 87s - loss: 0.1418 - acc: 0.9449 - val_loss: 0.2188 - val_acc: 0.9262

Epoch 00017: val_loss did not improve from 0.21018
Epoch 18/1000
 - 86s - loss: 0.1259 - acc: 0.9512 - val_loss: 0.2316 - val_acc: 0.9287

Epoch 00018: val_loss did not improve from 0.21018
Epoch 19/1000
 - 87s - loss: 0.1258 - acc: 0.9516 - val_loss: 0.2817 - val_acc: 0.9161

Epoch 00019: val_loss did not improve from 0.21018
Epoch 20/1000
 - 88s - loss: 0.1156 - acc: 0.9552 - val_loss: 0.2847 - val_acc: 0.9128

Epoch 00020: val_loss did not improve from 0.21018
Epoch 21/1000
 - 86s - loss: 0.1036 - acc: 0.9600 - val_loss: 0.2437 - val_acc: 0.9276

Epoch 00021: val_loss did not improve from 0.21018
Epoch 22/1000
 - 87s - loss: 0.0946 - acc: 0.9639 - val_loss: 0.2481 - val_acc: 0.9298

Epoch 00022: val_loss did not improve from 0.21018
Epoch 23/1000
 - 87s - loss: 0.0866 - acc: 0.9666 - val_loss: 0.2500 - val_acc: 0.9281

Epoch 00023: val_loss did not improve from 0.21018
Epoch 24/1000
 - 86s - loss: 0.0919 - acc: 0.9650 - val_loss: 0.2487 - val_acc: 0.9318

Epoch 00024: val_loss did not improve from 0.21018
Epoch 25/1000
 - 87s - loss: 0.0804 - acc: 0.9695 - val_loss: 0.2383 - val_acc: 0.9308

Epoch 00025: val_loss did not improve from 0.21018
Epoch 26/1000
 - 86s - loss: 0.0784 - acc: 0.9704 - val_loss: 0.2483 - val_acc: 0.9257

Epoch 00026: val_loss did not improve from 0.21018
Epoch 27/1000
 - 87s - loss: 0.0692 - acc: 0.9737 - val_loss: 0.2120 - val_acc: 0.9307

Epoch 00027: val_loss did not improve from 0.21018
Epoch 28/1000
 - 88s - loss: 0.0706 - acc: 0.9734 - val_loss: 0.2412 - val_acc: 0.9297

Epoch 00028: val_loss did not improve from 0.21018
Epoch 29/1000
 - 87s - loss: 0.0618 - acc: 0.9767 - val_loss: 0.2589 - val_acc: 0.9316

Epoch 00029: val_loss did not improve from 0.21018
Epoch 30/1000
 - 87s - loss: 0.0574 - acc: 0.9784 - val_loss: 0.2413 - val_acc: 0.9312

Epoch 00030: val_loss did not improve from 0.21018
Epoch 31/1000
 - 87s - loss: 0.0514 - acc: 0.9803 - val_loss: 0.2755 - val_acc: 0.9285

Epoch 00031: val_loss did not improve from 0.21018
Epoch 32/1000
 - 86s - loss: 0.0453 - acc: 0.9825 - val_loss: 0.2778 - val_acc: 0.9319

Epoch 00032: val_loss did not improve from 0.21018
Epoch 33/1000
 - 87s - loss: 0.0542 - acc: 0.9797 - val_loss: 0.2712 - val_acc: 0.9322

Epoch 00033: val_loss did not improve from 0.21018
Epoch 34/1000
 - 88s - loss: 0.0432 - acc: 0.9832 - val_loss: 0.2933 - val_acc: 0.9340

Epoch 00034: val_loss did not improve from 0.21018
Epoch 35/1000
 - 86s - loss: 0.0399 - acc: 0.9846 - val_loss: 0.2596 - val_acc: 0.9345

Epoch 00035: val_loss did not improve from 0.21018
Epoch 36/1000
 - 86s - loss: 0.0441 - acc: 0.9833 - val_loss: 0.2855 - val_acc: 0.9346

Epoch 00036: val_loss did not improve from 0.21018
Epoch 37/1000
 - 88s - loss: 0.0430 - acc: 0.9838 - val_loss: 0.3072 - val_acc: 0.9337

Epoch 00037: val_loss did not improve from 0.21018
Epoch 38/1000
 - 88s - loss: 0.0364 - acc: 0.9857 - val_loss: 0.2892 - val_acc: 0.9334

Epoch 00038: val_loss did not improve from 0.21018
Epoch 39/1000
 - 86s - loss: 0.0330 - acc: 0.9872 - val_loss: 0.3122 - val_acc: 0.9362

Epoch 00039: val_loss did not improve from 0.21018
Epoch 40/1000
 - 86s - loss: 0.0360 - acc: 0.9865 - val_loss: 0.3053 - val_acc: 0.9326

Epoch 00040: val_loss did not improve from 0.21018
Epoch 41/1000
 - 87s - loss: 0.0326 - acc: 0.9874 - val_loss: 0.3091 - val_acc: 0.9365

Epoch 00041: val_loss did not improve from 0.21018
Epoch 42/1000
 - 87s - loss: 0.0353 - acc: 0.9866 - val_loss: 0.2939 - val_acc: 0.9370

Epoch 00042: val_loss did not improve from 0.21018
Epoch 43/1000
 - 87s - loss: 0.0342 - acc: 0.9871 - val_loss: 0.3253 - val_acc: 0.9373

Epoch 00043: val_loss did not improve from 0.21018
Epoch 44/1000
 - 88s - loss: 0.0263 - acc: 0.9897 - val_loss: 0.3265 - val_acc: 0.9341

Epoch 00044: val_loss did not improve from 0.21018
Epoch 45/1000
 - 86s - loss: 0.0256 - acc: 0.9900 - val_loss: 0.3020 - val_acc: 0.9354

Epoch 00045: val_loss did not improve from 0.21018
Epoch 46/1000
 - 87s - loss: 0.0259 - acc: 0.9900 - val_loss: 0.3263 - val_acc: 0.9362

Epoch 00046: val_loss did not improve from 0.21018
Epoch 47/1000
 - 86s - loss: 0.0304 - acc: 0.9885 - val_loss: 0.3379 - val_acc: 0.9357

Epoch 00047: val_loss did not improve from 0.21018
Epoch 48/1000
 - 87s - loss: 0.0271 - acc: 0.9895 - val_loss: 0.3561 - val_acc: 0.9226

Epoch 00048: val_loss did not improve from 0.21018
Epoch 49/1000
 - 87s - loss: 0.0315 - acc: 0.9881 - val_loss: 0.3018 - val_acc: 0.9370

Epoch 00049: val_loss did not improve from 0.21018
Epoch 50/1000
 - 86s - loss: 0.0201 - acc: 0.9919 - val_loss: 0.3680 - val_acc: 0.9361

Epoch 00050: val_loss did not improve from 0.21018
Epoch 51/1000
 - 86s - loss: 0.0233 - acc: 0.9911 - val_loss: 0.2782 - val_acc: 0.9335

Epoch 00051: val_loss did not improve from 0.21018
Epoch 52/1000
 - 88s - loss: 0.0306 - acc: 0.9887 - val_loss: 0.3132 - val_acc: 0.9368

Epoch 00052: val_loss did not improve from 0.21018
Epoch 53/1000
 - 87s - loss: 0.0235 - acc: 0.9911 - val_loss: 0.3301 - val_acc: 0.9365

Epoch 00053: val_loss did not improve from 0.21018
Epoch 54/1000
 - 87s - loss: 0.0182 - acc: 0.9927 - val_loss: 0.3834 - val_acc: 0.9309

Epoch 00054: val_loss did not improve from 0.21018
Epoch 55/1000
 - 86s - loss: 0.0275 - acc: 0.9897 - val_loss: 0.3908 - val_acc: 0.9334

Epoch 00055: val_loss did not improve from 0.21018
Epoch 56/1000
 - 89s - loss: 0.0217 - acc: 0.9915 - val_loss: 0.3302 - val_acc: 0.9377

Epoch 00056: val_loss did not improve from 0.21018
Epoch 57/1000
 - 87s - loss: 0.0183 - acc: 0.9928 - val_loss: 0.3382 - val_acc: 0.9410

Epoch 00057: val_loss did not improve from 0.21018
Epoch 58/1000
 - 86s - loss: 0.0144 - acc: 0.9941 - val_loss: 0.3489 - val_acc: 0.9394

Epoch 00058: val_loss did not improve from 0.21018
Epoch 59/1000
 - 87s - loss: 0.0170 - acc: 0.9932 - val_loss: 0.3524 - val_acc: 0.9322

Epoch 00059: val_loss did not improve from 0.21018
Epoch 60/1000
 - 88s - loss: 0.0192 - acc: 0.9924 - val_loss: 0.3375 - val_acc: 0.9380

Epoch 00060: val_loss did not improve from 0.21018
Epoch 61/1000
 - 88s - loss: 0.0159 - acc: 0.9936 - val_loss: 0.3913 - val_acc: 0.9334

Epoch 00061: val_loss did not improve from 0.21018
Epoch 62/1000
 - 87s - loss: 0.0138 - acc: 0.9944 - val_loss: 0.3536 - val_acc: 0.9406

Epoch 00062: val_loss did not improve from 0.21018
Epoch 63/1000
 - 87s - loss: 0.0177 - acc: 0.9931 - val_loss: 0.3454 - val_acc: 0.9353

Epoch 00063: val_loss did not improve from 0.21018
Epoch 64/1000
 - 87s - loss: 0.0442 - acc: 0.9847 - val_loss: 0.2745 - val_acc: 0.9374

Epoch 00064: val_loss did not improve from 0.21018
Epoch 65/1000
 - 86s - loss: 0.0159 - acc: 0.9936 - val_loss: 0.3390 - val_acc: 0.9385

Epoch 00065: val_loss did not improve from 0.21018
Epoch 66/1000
 - 85s - loss: 0.0133 - acc: 0.9947 - val_loss: 0.3314 - val_acc: 0.9377

Epoch 00066: val_loss did not improve from 0.21018
Epoch 67/1000
 - 87s - loss: 0.0164 - acc: 0.9936 - val_loss: 0.3331 - val_acc: 0.9360

Epoch 00067: val_loss did not improve from 0.21018
Epoch 68/1000
 - 86s - loss: 0.0151 - acc: 0.9941 - val_loss: 0.3490 - val_acc: 0.9376

Epoch 00068: val_loss did not improve from 0.21018
Epoch 69/1000
 - 87s - loss: 0.0171 - acc: 0.9933 - val_loss: 0.3591 - val_acc: 0.9366

Epoch 00069: val_loss did not improve from 0.21018
Epoch 70/1000
 - 87s - loss: 0.0132 - acc: 0.9947 - val_loss: 0.3562 - val_acc: 0.9403

Epoch 00070: val_loss did not improve from 0.21018
Epoch 71/1000
 - 87s - loss: 0.0114 - acc: 0.9953 - val_loss: 0.3276 - val_acc: 0.9407

Epoch 00071: val_loss did not improve from 0.21018
Epoch 72/1000
 - 87s - loss: 0.0181 - acc: 0.9933 - val_loss: 0.3770 - val_acc: 0.9305

Epoch 00072: val_loss did not improve from 0.21018
Epoch 73/1000
 - 87s - loss: 0.0164 - acc: 0.9936 - val_loss: 0.3361 - val_acc: 0.9373

Epoch 00073: val_loss did not improve from 0.21018
Epoch 74/1000
 - 86s - loss: 0.0118 - acc: 0.9952 - val_loss: 0.3311 - val_acc: 0.9410

Epoch 00074: val_loss did not improve from 0.21018
Epoch 75/1000
 - 87s - loss: 0.0108 - acc: 0.9956 - val_loss: 0.3605 - val_acc: 0.9413

Epoch 00075: val_loss did not improve from 0.21018
Epoch 76/1000
 - 88s - loss: 0.0115 - acc: 0.9953 - val_loss: 0.3175 - val_acc: 0.9395

Epoch 00076: val_loss did not improve from 0.21018
Epoch 77/1000
 - 86s - loss: 0.0198 - acc: 0.9925 - val_loss: 0.3376 - val_acc: 0.9389

Epoch 00077: val_loss did not improve from 0.21018
Epoch 78/1000
 - 87s - loss: 0.0142 - acc: 0.9944 - val_loss: 0.3691 - val_acc: 0.9414

Epoch 00078: val_loss did not improve from 0.21018
Epoch 79/1000
 - 87s - loss: 0.0101 - acc: 0.9959 - val_loss: 0.3547 - val_acc: 0.9409

Epoch 00079: val_loss did not improve from 0.21018
Epoch 80/1000
 - 88s - loss: 0.0097 - acc: 0.9960 - val_loss: 0.3684 - val_acc: 0.9416

Epoch 00080: val_loss did not improve from 0.21018
Epoch 81/1000
 - 87s - loss: 0.0166 - acc: 0.9937 - val_loss: 0.3754 - val_acc: 0.9405

Epoch 00081: val_loss did not improve from 0.21018
Epoch 82/1000
 - 87s - loss: 0.0104 - acc: 0.9958 - val_loss: 0.3821 - val_acc: 0.9401

Epoch 00082: val_loss did not improve from 0.21018
Epoch 83/1000
 - 86s - loss: 0.0101 - acc: 0.9959 - val_loss: 0.3754 - val_acc: 0.9416

Epoch 00083: val_loss did not improve from 0.21018
Epoch 84/1000
 - 86s - loss: 0.0108 - acc: 0.9957 - val_loss: 0.3594 - val_acc: 0.9407

Epoch 00084: val_loss did not improve from 0.21018
Epoch 85/1000
 - 87s - loss: 0.0166 - acc: 0.9936 - val_loss: 0.3885 - val_acc: 0.9364

Epoch 00085: val_loss did not improve from 0.21018

Epoch 00085: ReduceLROnPlateau reducing learning rate to 4.99999987369e-05.
Epoch 86/1000
 - 86s - loss: 0.0101 - acc: 0.9959 - val_loss: 0.3667 - val_acc: 0.9408

Epoch 00086: val_loss did not improve from 0.21018
Epoch 87/1000
 - 87s - loss: 0.0077 - acc: 0.9968 - val_loss: 0.3848 - val_acc: 0.9418

Epoch 00087: val_loss did not improve from 0.21018
Epoch 88/1000
 - 88s - loss: 0.0072 - acc: 0.9970 - val_loss: 0.3758 - val_acc: 0.9419

Epoch 00088: val_loss did not improve from 0.21018
Epoch 89/1000
 - 90s - loss: 0.0067 - acc: 0.9972 - val_loss: 0.4018 - val_acc: 0.9425

Epoch 00089: val_loss did not improve from 0.21018
Epoch 90/1000
 - 86s - loss: 0.0071 - acc: 0.9971 - val_loss: 0.4117 - val_acc: 0.9411

Epoch 00090: val_loss did not improve from 0.21018
Epoch 91/1000
 - 86s - loss: 0.0073 - acc: 0.9970 - val_loss: 0.3787 - val_acc: 0.9432

Epoch 00091: val_loss did not improve from 0.21018
Epoch 92/1000
 - 87s - loss: 0.0068 - acc: 0.9972 - val_loss: 0.3723 - val_acc: 0.9418

Epoch 00092: val_loss did not improve from 0.21018
Epoch 93/1000
 - 87s - loss: 0.0064 - acc: 0.9973 - val_loss: 0.3954 - val_acc: 0.9426

Epoch 00093: val_loss did not improve from 0.21018
Epoch 94/1000
 - 86s - loss: 0.0061 - acc: 0.9974 - val_loss: 0.4198 - val_acc: 0.9416

Epoch 00094: val_loss did not improve from 0.21018
Epoch 95/1000
 - 86s - loss: 0.0064 - acc: 0.9973 - val_loss: 0.4024 - val_acc: 0.9429

Epoch 00095: val_loss did not improve from 0.21018
Epoch 96/1000
 - 86s - loss: 0.0068 - acc: 0.9972 - val_loss: 0.3938 - val_acc: 0.9418

Epoch 00096: val_loss did not improve from 0.21018
Epoch 97/1000
 - 87s - loss: 0.0066 - acc: 0.9973 - val_loss: 0.3807 - val_acc: 0.9407

Epoch 00097: val_loss did not improve from 0.21018
Epoch 98/1000
 - 86s - loss: 0.0062 - acc: 0.9974 - val_loss: 0.3917 - val_acc: 0.9419

Epoch 00098: val_loss did not improve from 0.21018
Epoch 99/1000
 - 86s - loss: 0.0058 - acc: 0.9976 - val_loss: 0.4323 - val_acc: 0.9410

Epoch 00099: val_loss did not improve from 0.21018
Epoch 100/1000
 - 88s - loss: 0.0057 - acc: 0.9976 - val_loss: 0.4272 - val_acc: 0.9410

Epoch 00100: val_loss did not improve from 0.21018
Epoch 101/1000
 - 86s - loss: 0.0063 - acc: 0.9975 - val_loss: 0.4282 - val_acc: 0.9409

Epoch 00101: val_loss did not improve from 0.21018
Epoch 102/1000
 - 88s - loss: 0.0055 - acc: 0.9977 - val_loss: 0.4317 - val_acc: 0.9413

Epoch 00102: val_loss did not improve from 0.21018
Epoch 103/1000
 - 86s - loss: 0.0055 - acc: 0.9977 - val_loss: 0.4386 - val_acc: 0.9417

Epoch 00103: val_loss did not improve from 0.21018
Epoch 00103: early stopping
Finished training

Date and time: 2019-01-11 17-45-10

